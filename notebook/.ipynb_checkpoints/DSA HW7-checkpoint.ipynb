{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSCI 552 | Machine Learning for Data Science\n",
    "\n",
    "Homework 7\n",
    "\n",
    "Matheus Schmitz\n",
    "\n",
    "USC ID: 5039286453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in c:\\users\\matheus\\anaconda3\\lib\\site-packages (0.10.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\matheus\\anaconda3\\lib\\site-packages (from tensorflow_addons) (2.9.1)\n"
     ]
    }
   ],
   "source": [
    "# Need tensorflow_addons to use AdamW optimizer\n",
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Py Data Stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Tensor Flow & Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "# String Manipulation\n",
    "import string\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-46d45dd0bd84>:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "TensorFlow using GPU: True\n",
      "TensorFlow using CUDA: True\n",
      "Num GPUs Available:  1\n",
      "Default GPU Device: /device:GPU:0\n",
      "\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:12:52_Pacific_Daylight_Time_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n",
      "\n",
      "Sun Nov 08 14:00:57 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 446.14       Driver Version: 446.14       CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 960M   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   50C    P0    N/A /  N/A |    155MiB /  2048MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU                  PID   Type   Process name                  GPU Memory |\n",
      "|                                                                  Usage      |\n",
      "|=============================================================================|\n",
      "|    0                 2288      C   ...heus\\Anaconda3\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Making sure Tensor Flow is properly working with GPU\n",
    "print('Available Devices:')\n",
    "for device in tf.config.experimental.list_physical_devices():\n",
    "    print(device)\n",
    "print()\n",
    "print(f'TensorFlow using GPU: {tf.test.is_gpu_available()}')\n",
    "print(f'TensorFlow using CUDA: {tf.test.is_built_with_cuda()}')\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Oh boy, there's no GPU, so prepare yourself for a long wait :(\")\n",
    "print()\n",
    "try:\n",
    "    !nvcc --version\n",
    "except:\n",
    "    print('ooops, watch out, something went wrong!')\n",
    "print()\n",
    "try:\n",
    "    !nvidia-smi\n",
    "except:\n",
    "    print('ooops, watch out, something went wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Project Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AIIMAT.txt',\n",
       " 'MLOE.txt',\n",
       " 'OKEWFSMP.txt',\n",
       " 'TAM.txt',\n",
       " 'TAMatter.txt',\n",
       " 'THWP.txt',\n",
       " 'TPP.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOOKS_PATH = '../data/books/'\n",
    "BOOK_TXTS = os.listdir(BOOKS_PATH)\n",
    "BOOK_TXTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) LSTM Mimicking Russell's Style and Thoughts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i) Create Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5095252 characters in the corpus\n"
     ]
    }
   ],
   "source": [
    "# Path to the corpus \n",
    "CORPUS_PATH = 'Corpus.txt'\n",
    "\n",
    "# Create a single corpus with all files\n",
    "with open(CORPUS_PATH, 'w') as concatenated_corpus:\n",
    "    for book in BOOK_TXTS:\n",
    "        with open(BOOKS_PATH+book, 'r', encoding='ascii', errors='ignore') as txt_file:\n",
    "            for line in txt_file:\n",
    "                concatenated_corpus.write(line)\n",
    "                \n",
    "# Store the corpus in a file\n",
    "CORPUS = open(CORPUS_PATH, 'r').read()\n",
    "print(f'There are {len(CORPUS)} characters in the corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ii) Character-Level Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note 2 print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the corpus\n",
    "CORPUS_CLEAN = np.copy(CORPUS).tolist()\n",
    "CORPUS_CLEAN = CORPUS_CLEAN.lower()\n",
    "CORPUS_CLEAN = CORPUS_CLEAN.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Clean some memory - it'll be needed\n",
    "del CORPUS\n",
    "\n",
    "# Get all unique characters for mapping to extended ENCODING\n",
    "UNIQUE_CHARS = set(CORPUS_CLEAN)\n",
    "\n",
    "# Dictionaries to store the character-ENCODING mappings\n",
    "CHAR_to_ENCODING = {}\n",
    "ENCODING_to_CHAR = {}\n",
    "CHAR_to_ENCODING_normalized = {}\n",
    "ENCODING_to_CHAR_normalized = {}\n",
    "\n",
    "# Create ENCODING encodings\n",
    "VECTOR_LENGTH = len(UNIQUE_CHARS)-1\n",
    "for index, char in enumerate(sorted(UNIQUE_CHARS)):\n",
    "    CHAR_to_ENCODING[char] = index\n",
    "    ENCODING_to_CHAR[index] = char\n",
    "    CHAR_to_ENCODING_normalized[char] = index/VECTOR_LENGTH\n",
    "    ENCODING_to_CHAR_normalized[index/VECTOR_LENGTH] = char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iii) Define Window Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iv) Generating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 4942255/4942255 [02:13<00:00, 36911.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lists for the features and labels\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Loop through the CORPUS_CLEAN generating features\n",
    "for i in tqdm(range(len(CORPUS_CLEAN)-WINDOW_SIZE)):\n",
    "    \n",
    "    # Extract the X and Y characters\n",
    "    x_chars = CORPUS_CLEAN[i: i+WINDOW_SIZE-1]\n",
    "    y_char = CORPUS_CLEAN[i+WINDOW_SIZE-1]\n",
    "    \n",
    "    # Encode X to the normalized [0,1] range and Y to the unnormalized range (so that Y can later be one-hot encoded)\n",
    "    x_encodings = [CHAR_to_ENCODING_normalized[char] for char in x_chars]\n",
    "    y_encoding = [CHAR_to_ENCODING[char] for char in y_char]\n",
    "    \n",
    "    # Add the feature and label to X and Y\n",
    "    X.append(x_encodings)\n",
    "    Y.append(y_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (v) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4942255, 38)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Onehot encode Y\n",
    "Y_onehot = tf.keras.utils.to_categorical(Y)\n",
    "Y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Need to reshape X so it matches Y's shape\n",
    "X = np.reshape(X, (Y_onehot.shape[0], WINDOW_SIZE-1, 1))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (vi) / (vii) Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vii\n",
    "note 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURONS = 52\n",
    "\n",
    "# Instantiate Keras Sequential Model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# LSTM Layer\n",
    "model.add(LSTM(units=NEURONS, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
    "\n",
    "# Softmax layer\n",
    "model.add(Dense(Y_onehot.shape[1], activation='softmax'))\n",
    "\n",
    "# Objective Function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_crossentropy'])\n",
    "\n",
    "# Check model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (viii) / (ix) / (x) Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "viii\n",
    "\n",
    "ix \n",
    "\n",
    "x\n",
    "\n",
    "note 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define directory for model checkpoints\n",
    "BACKUP_DIR = './checkpoints'\n",
    "if not os.path.exists(BACKUP_DIR):\n",
    "    os.mkdir(BACKUP_DIR)\n",
    "    \n",
    "# Define file to store checkpoint\n",
    "BACKUP_FILE = os.path.join(BACKUP_DIR, 'model_1.hdf5')\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(BACKUP_FILE, \n",
    "                             monitor='loss',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=1)\n",
    "plateauLRreduce = ReduceLROnPlateau(factor = 0.1,\n",
    "                                    patience = 5,\n",
    "                                    monitor='loss',\n",
    "                                    min_lr = 0.0000001,\n",
    "                                    verbose=1)\n",
    "stopearly = EarlyStopping(monitor='loss',\n",
    "                          patience=15,\n",
    "                          verbose=1)\n",
    "logCSV = CSVLogger(filename='model_log',\n",
    "                   separator=',', \n",
    "                   append=False)\n",
    "\n",
    "model_callbacks = [checkpoint, plateauLRreduce, stopearly, logCSV]\n",
    "\n",
    "# Train model and save history\n",
    "model_history = model.fit(X,\n",
    "                          Y_onehot,\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          callbacks=model_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (xi) Generate 1000 Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xi print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights\n",
    "model.load_weights('./checkpoints/model_1.hdf5')\n",
    "\n",
    "# Objective Function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'categorical_crossentropy'])\n",
    "\n",
    "# Check model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base text\n",
    "base_text = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'\n",
    "print(f'Lenght of base test before pre-processing: {len(base_text)}')\n",
    "\n",
    "### PRE-PROCESSING ###\n",
    "\n",
    "# Clean the corpus\n",
    "base_text = np.copy(base_text).tolist()\n",
    "base_text = base_text.lower()\n",
    "base_text = base_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Encode\n",
    "x_chars = base_text[0: len(base_text)]\n",
    "\n",
    "# Encode X to the normalized [0,1] range and Y to the unnormalized range (so that Y can later be one-hot encoded)\n",
    "x_encodings = [CHAR_to_ENCODING_normalized[char] for char in x_chars]\n",
    "\n",
    "# Add the feature to X_test\n",
    "X_text = x_encodings\n",
    "\n",
    "# Reshape\n",
    "# Need to reshape X so it matches Y's shape\n",
    "X_text = np.reshape(x_encodings, (1, len(x_encodings), 1))\n",
    "\n",
    "print(f'Lenght of base test after pre-processing: {X_text.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating text\n",
    "synthetic_text = X_text.copy()\n",
    "synthetic_text = synthetic_text.flatten()\n",
    "\n",
    "# Generate 1000 words\n",
    "for i in tqdm(range(1000)):\n",
    "    \n",
    "    # Get synthetic_text to the needed input shape\n",
    "    input_text = np.reshape(synthetic_text[-99:], (1, WINDOW_SIZE-1, 1))\n",
    "    \n",
    "    # Predict a character\n",
    "    pred = model.predict(input_text)\n",
    "    \n",
    "    # Select the index of the most likely character\n",
    "    pred_idx = np.argmax(pred)\n",
    "    \n",
    "    # Use that index to retrieve the character\n",
    "    pred_normalized = pred_idx/VECTOR_LENGTH\n",
    "    \n",
    "    # Add the prediction to the synthetic text\n",
    "    synthetic_text = np.append(synthetic_text, pred_normalized)\n",
    "\n",
    "# Once all characters have been predited, transform back to letters\n",
    "predicted_text = [ENCODING_to_CHAR_normalized[encoding] for encoding in synthetic_text]\n",
    "\n",
    "# Then concatenate all letters to form the text\n",
    "predicted_text = ''.join(predicted_text)\n",
    "\n",
    "# Result\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (xii) Deeper Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xii print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURONS = 256\n",
    "\n",
    "# Instantiate Keras Sequential Model\n",
    "model2 = keras.models.Sequential()\n",
    "\n",
    "# LSTM Layer 1\n",
    "model2.add(LSTM(units=NEURONS, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "\n",
    "# Dropout Layer\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "# LSTM Layer 2\n",
    "model2.add(LSTM(units=NEURONS, return_sequences=True))\n",
    "\n",
    "# LSTM Layer 3\n",
    "model2.add(LSTM(units=NEURONS, return_sequences=False))\n",
    "\n",
    "# Dropout Layer\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "# Softmax layer\n",
    "model2.add(Dense(Y_onehot.shape[1], activation='softmax'))\n",
    "\n",
    "# Objective Function\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy', 'categorical_crossentropy'])\n",
    "\n",
    "# Check model\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Define directory for model checkpoints\n",
    "BACKUP_DIR = './checkpoints'\n",
    "if not os.path.exists(BACKUP_DIR):\n",
    "    os.mkdir(BACKUP_DIR)\n",
    "    \n",
    "# Define file to store checkpoint\n",
    "BACKUP_FILE = os.path.join(BACKUP_DIR, 'model_2.hdf5')\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(BACKUP_FILE, \n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             verbose=1)\n",
    "plateauLRreduce = ReduceLROnPlateau(factor = 0.1,\n",
    "                                    patience = 5,\n",
    "                                    monitor='loss',\n",
    "                                    min_lr = 0.0000001,\n",
    "                                    verbose=1)\n",
    "stopearly = EarlyStopping(monitor='val_loss',\n",
    "                          patience=15,\n",
    "                          verbose=1)\n",
    "logCSV = CSVLogger(filename='model2_log',\n",
    "                   separator=',', \n",
    "                   append=False)\n",
    "\n",
    "model_callbacks = [checkpoint, plateauLRreduce, stopearly, logCSV]\n",
    "\n",
    "# Train model and save history\n",
    "#model_history = model2.fit(X,\n",
    "                           Y_onehot,\n",
    "                           epochs=EPOCHS,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           callbacks=model_callbacks,\n",
    "                           validation_split=0.2,\n",
    "                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
